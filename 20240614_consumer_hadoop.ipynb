{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e73dcb4-b94b-4556-9ea0-a2bd7f6529f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "from pyarrow import fs\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "conf = {'bootstrap.servers': \"192.168.0.201:9092\", \n",
    "        'group.id' : 'test-group',\n",
    "         'auto.offset.reset' : 'earliest'}\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['RSH_KAFKA'])\n",
    "\n",
    "classpath = subprocess.Popen([\"/home/hadoop/hadoop/bin/hdfs\", \"classpath\", \"--glob\"],\n",
    "                 stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ['CLASSPATH'] = classpath.decode('utf-8')\n",
    "os.environ['ARROW_LIBHDFS_DIR'] = \"/home/hadoop/hadoop/lib/native\"\n",
    "hdfs = fs.HadoopFileSystem(host='192.168.0.160', port=8020, user='hadoop')\n",
    "\n",
    "\n",
    "# Initialize DataFrame to store messages\n",
    "df = pd.DataFrame()\n",
    "next_offset_threshold = 100\n",
    "\n",
    "def process_message(message):\n",
    "    global df, next_offset_threshold\n",
    "    item_json_str = message.value().decode('utf-8')\n",
    "    item = json.loads(item_json_str)\n",
    "    \n",
    "    # Convert item to DataFrame and concatenate with existing DataFrame\n",
    "    item_df = pd.DataFrame([item])\n",
    "    df = pd.concat([df, item_df], ignore_index=True)\n",
    "    \n",
    "    # Log the addition of a new item\n",
    "    print(f\"Added message to DataFrame: offset={message.offset()}\")\n",
    "\n",
    "    # Check the message offset and write to HDFS at specific offsets\n",
    "    if message.offset() >= next_offset_threshold:\n",
    "        write_to_hdfs(df, next_offset_threshold)\n",
    "        df = pd.DataFrame()  # Reset DataFrame\n",
    "        next_offset_threshold += 100  # Increase the threshold for the next batch\n",
    "\n",
    "\n",
    "def write_to_hdfs(dataframe, offset):\n",
    "    # Define HDFS file path for the current offset\n",
    "    hdfs_file_path = f\"/user/rsh/tmp2_offset_{offset}.csv\"\n",
    "    with hdfs.open_output_stream(hdfs_file_path) as fs_stream:\n",
    "        line = dataframe.to_csv(index=False)\n",
    "        fs_stream.write(line.encode())\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            break\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                print(f\"{msg.topic()} [{msg.partition()}] reached end at offset {msg.offset()}\")\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # Proper message\n",
    "            process_message(msg)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer interrupted by user.\")\n",
    "finally:\n",
    "    # Write remaining messages for the last set to HDFS\n",
    "    if not df.empty:\n",
    "        write_to_hdfs(df, 'final')\n",
    "    \n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()\n",
    "    print(\"Consumer closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4350940-22d3-47dd-a86b-7039679ae5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': \"192.168.0.201:9092\", \n",
    "        'group.id' : 'test-group',\n",
    "         'auto.offset.reset' : 'earliest'}\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['RSH_KAFKA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45718b53-25dc-4019-882d-cb2ae0c1cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "classpath = subprocess.Popen([\"/home/hadoop/hadoop/bin/hdfs\", \"classpath\", \"--glob\"],\n",
    "                 stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ['CLASSPATH'] = classpath.decode('utf-8')\n",
    "os.environ['ARROW_LIBHDFS_DIR'] = \"/home/hadoop/hadoop/lib/native\"\n",
    "hdfs = fs.HadoopFileSystem(host='192.168.0.160', port=8020, user='hadoop')\n",
    "\n",
    "\n",
    "# Initialize DataFrame to store messages\n",
    "df = pd.DataFrame()\n",
    "next_offset_threshold = 100\n",
    "\n",
    "def process_message(message):\n",
    "    global df, next_offset_threshold\n",
    "    item_json_str = message.value().decode('utf-8')\n",
    "    item = json.loads(item_json_str)\n",
    "    \n",
    "    # Convert item to DataFrame and concatenate with existing DataFrame\n",
    "    item_df = pd.DataFrame([item])\n",
    "    df = pd.concat([df, item_df], ignore_index=True)\n",
    "    \n",
    "    # Log the addition of a new item\n",
    "    print(f\"Added message to DataFrame: offset={message.offset()}\")\n",
    "\n",
    "    # Check the message offset and write to HDFS at specific offsets\n",
    "    if message.offset() >= next_offset_threshold:\n",
    "        write_to_hdfs(df, next_offset_threshold)\n",
    "        df = pd.DataFrame()  # Reset DataFrame\n",
    "        next_offset_threshold += 100  # Increase the threshold for the next batch\n",
    "\n",
    "\n",
    "def write_to_hdfs(dataframe, offset):\n",
    "    # Define HDFS file path for the current offset\n",
    "    hdfs_file_path = f\"/user/rsh/tmp2_offset_{offset}.csv\"\n",
    "    with hdfs.open_output_stream(hdfs_file_path) as fs_stream:\n",
    "        line = dataframe.to_csv(index=False)\n",
    "        fs_stream.write(line.encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52f8a7-b526-4363-9b86-31bd3881f78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            break\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                print(f\"{msg.topic()} [{msg.partition()}] reached end at offset {msg.offset()}\")\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # Proper message\n",
    "            process_message(msg)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer interrupted by user.\")\n",
    "finally:\n",
    "    # Write remaining messages for the last set to HDFS\n",
    "    if not df.empty:\n",
    "        write_to_hdfs(df, 'final')\n",
    "    \n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()\n",
    "    print(\"Consumer closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcb36a-612b-4d25-ba6b-2bfa5866d3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
