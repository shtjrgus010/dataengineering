{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfa101d-6cc7-4109-9bd3-3eebebee4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1318d2b4-ee75-4555-b1b6-556285cb1b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hflight = spark.read.option('inferSchema', 'true').\\\n",
    "    option('header', 'true').\\\n",
    "    csv(\"file:///home/hadoop/jupyter/dataengineering/dataset/hflight.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748946ba-9fd5-455d-b89e-c9e1e40d0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "227496"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hflight.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717985be-af7a-494d-b801-445e5d0d0a3f",
   "metadata": {},
   "source": [
    "## Pyspark 버전\n",
    "### 출발 공항에 대해서 도착 공항별로 평균 출발 지연, 평균 도착 지연시간을 구하기!\n",
    "#### groupby\n",
    "#### avg,mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f8c9375-2c9a-41b7-beb7-f936f59fde94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|ArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|\n",
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|2011|    1|         1|        6|   1400|   1500|           AA|      428| N576AA|               60|     40|     -10|       0|   IAH| DFW|     224|     7|     13|        0|            null|       0|\n",
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hflight.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1fbf0ca-e787-4ddf-898c-85092f7693e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      " |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      " |\n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |\n",
      " |  .. versionadded:: 1.3.0\n",
      " |\n",
      " |  .. versionchanged:: 3.4.0\n",
      " |      Supports Spark Connect.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`:\n",
      " |\n",
      " |  >>> people = spark.createDataFrame([\n",
      " |  ...     {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
      " |  ...     {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
      " |  ...     {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
      " |  ...     {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
      " |  ... ])\n",
      " |\n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |\n",
      " |  To select a column from the :class:`DataFrame`, use the apply method:\n",
      " |\n",
      " |  >>> age_col = people.age\n",
      " |\n",
      " |  A more concrete example:\n",
      " |\n",
      " |  >>> # To create DataFrame using SparkSession\n",
      " |  ... department = spark.createDataFrame([\n",
      " |  ...     {\"id\": 1, \"name\": \"PySpark\"},\n",
      " |  ...     {\"id\": 2, \"name\": \"ML\"},\n",
      " |  ...     {\"id\": 3, \"name\": \"Spark SQL\"}\n",
      " |  ... ])\n",
      " |\n",
      " |  >>> people.filter(people.age > 30).join(\n",
      " |  ...     department, people.deptId == department.id).groupBy(\n",
      " |  ...     department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
      " |  +-------+------+-----------+--------+\n",
      " |  |   name|gender|avg(salary)|max(age)|\n",
      " |  +-------+------+-----------+--------+\n",
      " |  |     ML|     F|      150.0|      60|\n",
      " |  |PySpark|     M|       75.0|      50|\n",
      " |  +-------+------+-----------+--------+\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  A DataFrame should only be created as described above. It should not be directly\n",
      " |  created via using the constructor.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Column name to return as :class:`Column`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |          Requested column.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |\n",
      " |      Retrieve a column instance.\n",
      " |\n",
      " |      >>> df.select(df.age).show()\n",
      " |      +---+\n",
      " |      |age|\n",
      " |      +---+\n",
      " |      |  2|\n",
      " |      |  5|\n",
      " |      +---+\n",
      " |\n",
      " |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      item : int, str, :class:`Column`, list or tuple\n",
      " |          column index, column name, column, or a list or tuple of columns\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column` or :class:`DataFrame`\n",
      " |          a specified column, or a filtered or projected dataframe.\n",
      " |\n",
      " |          * If the input `item` is an int or str, the output is a :class:`Column`.\n",
      " |\n",
      " |          * If the input `item` is a :class:`Column`, the output is a :class:`DataFrame`\n",
      " |              filtered by this given :class:`Column`.\n",
      " |\n",
      " |          * If the input `item` is a list or tuple, the output is a :class:`DataFrame`\n",
      " |              projected by this given list or tuple.\n",
      " |\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |\n",
      " |      Retrieve a column instance.\n",
      " |\n",
      " |      >>> df.select(df['age']).show()\n",
      " |      +---+\n",
      " |      |age|\n",
      " |      +---+\n",
      " |      |  2|\n",
      " |      |  5|\n",
      " |      +---+\n",
      " |\n",
      " |      >>> df.select(df[1]).show()\n",
      " |      +-----+\n",
      " |      | name|\n",
      " |      +-----+\n",
      " |      |Alice|\n",
      " |      |  Bob|\n",
      " |      +-----+\n",
      " |\n",
      " |      Select multiple string columns as index.\n",
      " |\n",
      " |      >>> df[[\"name\", \"age\"]].show()\n",
      " |      +-----+---+\n",
      " |      | name|age|\n",
      " |      +-----+---+\n",
      " |      |Alice|  2|\n",
      " |      |  Bob|  5|\n",
      " |      +-----+---+\n",
      " |      >>> df[df.age > 3].show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df[df[0] > 3].show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |\n",
      " |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy().agg()``).\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      exprs : :class:`Column` or dict of key and value strings\n",
      " |          Columns or expressions to aggregate DataFrame by.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Aggregated DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.agg({\"age\": \"max\"}).show()\n",
      " |      +--------+\n",
      " |      |max(age)|\n",
      " |      +--------+\n",
      " |      |       5|\n",
      " |      +--------+\n",
      " |      >>> df.agg(F.min(df.age)).show()\n",
      " |      +--------+\n",
      " |      |min(age)|\n",
      " |      +--------+\n",
      " |      |       2|\n",
      " |      +--------+\n",
      " |\n",
      " |  alias(self, alias: str) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          an alias name to be set for the :class:`DataFrame`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Aliased DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col, desc\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\n",
      " |      ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n",
      " |      +-----+-----+---+\n",
      " |      | name| name|age|\n",
      " |      +-----+-----+---+\n",
      " |      |  Tom|  Tom| 14|\n",
      " |      |  Bob|  Bob| 16|\n",
      " |      |Alice|Alice| 23|\n",
      " |      +-----+-----+---+\n",
      " |\n",
      " |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |\n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |\n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[https://doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |\n",
      " |      .. versionadded:: 2.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col: str, tuple or list\n",
      " |          Can be a single column name, or a list of names for multiple columns.\n",
      " |\n",
      " |          .. versionchanged:: 2.2.0\n",
      " |             Added support for multiple columns.\n",
      " |      probabilities : list or tuple\n",
      " |          a list of quantile probabilities\n",
      " |          Each number must belong to [0, 1].\n",
      " |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      relativeError : float\n",
      " |          The relative target precision to achieve\n",
      " |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |          could be very expensive. Note that values greater than 1 are\n",
      " |          accepted but gives the same result as 1.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the approximate quantiles at the given probabilities.\n",
      " |\n",
      " |          * If the input `col` is a string, the output is a list of floats.\n",
      " |\n",
      " |          * If the input `col` is a list or tuple of strings, the output is also a\n",
      " |              list, but each element in it is a list of floats, i.e., the output\n",
      " |              is a list of list of floats.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |\n",
      " |  cache(self) -> 'DataFrame'\n",
      " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK_DESER`).\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Cached DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> df.cache()\n",
      " |      DataFrame[id: bigint]\n",
      " |\n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      InMemoryTableScan ...\n",
      " |\n",
      " |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n",
      " |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
      " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
      " |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      " |\n",
      " |      .. versionadded:: 2.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional, default True\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Checkpointed DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import tempfile\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     spark.sparkContext.setCheckpointDir(\"/tmp/bb\")\n",
      " |      ...     df.checkpoint(False)\n",
      " |      DataFrame[age: bigint, name: string]\n",
      " |\n",
      " |  coalesce(self, numPartitions: int) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |\n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |\n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          specify the target number of partitions\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |\n",
      " |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |\n",
      " |      .. versionadded:: 2.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, column name specified as a regex.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |\n",
      " |  collect(self) -> List[pyspark.sql.types.Row]\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of rows.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.collect()\n",
      " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      " |\n",
      " |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |      method : str, optional\n",
      " |          The correlation method. Currently only supports \"pearson\"\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Pearson Correlation Coefficient of two columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      " |      >>> df.corr(\"c1\", \"c2\")\n",
      " |      -0.3592106040535498\n",
      " |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      " |      >>> df.corr(\"small\", \"bigger\")\n",
      " |      1.0\n",
      " |\n",
      " |  count(self) -> int\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Number of rows.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      Return the number of rows in the :class:`DataFrame`.\n",
      " |\n",
      " |      >>> df.count()\n",
      " |      3\n",
      " |\n",
      " |  cov(self, col1: str, col2: str) -> float\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Covariance of two columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      " |      >>> df.cov(\"c1\", \"c2\")\n",
      " |      -18.0\n",
      " |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      " |      >>> df.cov(\"small\", \"bigger\")\n",
      " |      1.0\n",
      " |\n",
      " |  createGlobalTempView(self, name: str) -> None\n",
      " |      Creates a global temporary view with this :class:`DataFrame`.\n",
      " |\n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |\n",
      " |      .. versionadded:: 2.1.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the view.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a global temporary view.\n",
      " |\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |\n",
      " |      Throws an exception if the global temporary view already exists.\n",
      " |\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      True\n",
      " |\n",
      " |  createOrReplaceGlobalTempView(self, name: str) -> None\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |\n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |\n",
      " |      .. versionadded:: 2.2.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the view.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a global temporary view.\n",
      " |\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |\n",
      " |      Replace the global temporary view.\n",
      " |\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      True\n",
      " |\n",
      " |  createOrReplaceTempView(self, name: str) -> None\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |\n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 2.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the view.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a local temporary view named 'people'.\n",
      " |\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |\n",
      " |      Replace the local temporary view.\n",
      " |\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"SELECT * FROM people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |\n",
      " |  createTempView(self, name: str) -> None\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |\n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |\n",
      " |      .. versionadded:: 2.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the view.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a local temporary view.\n",
      " |\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |\n",
      " |      Throw an exception if the table already exists.\n",
      " |\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |\n",
      " |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 2.1.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the cartesian product.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Joined DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df2 = spark.createDataFrame(\n",
      " |      ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n",
      " |      +---+-----+------+\n",
      " |      |age| name|height|\n",
      " |      +---+-----+------+\n",
      " |      | 14|  Tom|    80|\n",
      " |      | 14|  Tom|    85|\n",
      " |      | 23|Alice|    80|\n",
      " |      | 23|Alice|    85|\n",
      " |      | 16|  Bob|    80|\n",
      " |      | 16|  Bob|    85|\n",
      " |      +---+-----+------+\n",
      " |\n",
      " |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      col2 : str\n",
      " |          The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Frequency matrix of two columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      " |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
      " |      +-----+---+---+---+\n",
      " |      |c1_c2| 10| 11|  8|\n",
      " |      +-----+---+---+---+\n",
      " |      |    1|  0|  2|  0|\n",
      " |      |    3|  1|  0|  0|\n",
      " |      |    4|  0|  0|  2|\n",
      " |      +-----+---+---+---+\n",
      " |\n",
      " |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          columns to create cube by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      " |          or list of them.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`GroupedData`\n",
      " |          Cube of the data by given columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |\n",
      " |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |\n",
      " |      .. versionadded:: 1.3.1\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      This includes count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list, optional\n",
      " |           Column name or list of column names to describe by (default All columns).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          A new DataFrame that describes (provides statistics) given DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      " |      ... )\n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+----+\n",
      " |      |summary| age|\n",
      " |      +-------+----+\n",
      " |      |  count|   3|\n",
      " |      |   mean|12.0|\n",
      " |      | stddev| 1.0|\n",
      " |      |    min|  11|\n",
      " |      |    max|  13|\n",
      " |      +-------+----+\n",
      " |\n",
      " |      >>> df.describe(['age', 'weight', 'height']).show()\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |summary| age|            weight|           height|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |  count|   3|                 3|                3|\n",
      " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      " |      |    min|  11|              37.8|            142.2|\n",
      " |      |    max|  13|              44.1|            150.5|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.summary\n",
      " |\n",
      " |  distinct(self) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with distinct records.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      Return the number of distinct rows in the :class:`DataFrame`\n",
      " |\n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |\n",
      " |  drop(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` without specified columns.\n",
      " |      This is a no-op if the schema doesn't contain the given column name(s).\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols: str or :class:`Column`\n",
      " |          a name of the column, or the :class:`Column` to drop\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame without given columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      " |\n",
      " |      >>> df.drop('age').show()\n",
      " |      +-----+\n",
      " |      | name|\n",
      " |      +-----+\n",
      " |      |  Tom|\n",
      " |      |Alice|\n",
      " |      |  Bob|\n",
      " |      +-----+\n",
      " |      >>> df.drop(df.age).show()\n",
      " |      +-----+\n",
      " |      | name|\n",
      " |      +-----+\n",
      " |      |  Tom|\n",
      " |      |Alice|\n",
      " |      |  Bob|\n",
      " |      +-----+\n",
      " |\n",
      " |      Drop the column that joined both DataFrames on.\n",
      " |\n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
      " |      +---+------+\n",
      " |      |age|height|\n",
      " |      +---+------+\n",
      " |      | 14|    80|\n",
      " |      | 16|    85|\n",
      " |      +---+------+\n",
      " |\n",
      " |  dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |\n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and the system will accordingly limit the state. In addition, data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      subset : List of column names, optional\n",
      " |          List of columns to use for duplicate comparison (default All columns).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame without duplicates.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(name='Alice', age=5, height=80),\n",
      " |      ...     Row(name='Alice', age=5, height=80),\n",
      " |      ...     Row(name='Alice', age=10, height=80)\n",
      " |      ... ])\n",
      " |\n",
      " |      Deduplicate the same rows.\n",
      " |\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      |Alice| 10|    80|\n",
      " |      +-----+---+------+\n",
      " |\n",
      " |      Deduplicate values on 'name' and 'height' columns.\n",
      " |\n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      +-----+---+------+\n",
      " |\n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |\n",
      " |      .. versionadded:: 1.4\n",
      " |\n",
      " |  dropna(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |\n",
      " |      .. versionadded:: 1.3.1\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      how : str, optional\n",
      " |          'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      thresh: int, optional\n",
      " |          default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with null only rows excluded.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      " |      ...     Row(age=None, height=None, name=None),\n",
      " |      ... ])\n",
      " |      >>> df.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |\n",
      " |  exceptAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |\n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      As standard in SQL, this function resolves columns by position (not by name).\n",
      " |\n",
      " |      .. versionadded:: 2.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          The other :class:`DataFrame` to compare to.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |\n",
      " |  explain(self, extended: Union[bool, str, NoneType] = None, mode: Optional[str] = None) -> None\n",
      " |      Prints the (logical and physical) plans to the console for debugging purposes.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extended : bool, optional\n",
      " |          default ``False``. If ``False``, prints only the physical plan.\n",
      " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      " |          specified.\n",
      " |      mode : str, optional\n",
      " |          specifies the expected output format of plans.\n",
      " |\n",
      " |          * ``simple``: Print only a physical plan.\n",
      " |          * ``extended``: Print both logical and physical plans.\n",
      " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      " |\n",
      " |          .. versionchanged:: 3.0.0\n",
      " |             Added optional argument `mode` to specify the expected output format of plans.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      Print out the physical plan only (default).\n",
      " |\n",
      " |      >>> df.explain()  # doctest: +SKIP\n",
      " |      == Physical Plan ==\n",
      " |      *(1) Scan ExistingRDD[age...,name...]\n",
      " |\n",
      " |      Print out all of the parsed, analyzed, optimized and physical plans.\n",
      " |\n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |\n",
      " |      Print out the plans with two sections: a physical plan outline and node details\n",
      " |\n",
      " |      >>> df.explain(mode=\"formatted\")  # doctest: +SKIP\n",
      " |      == Physical Plan ==\n",
      " |      * Scan ExistingRDD (...)\n",
      " |      (1) Scan ExistingRDD [codegen id : ...]\n",
      " |      Output [2]: [age..., name...]\n",
      " |      ...\n",
      " |\n",
      " |      Print a logical plan and statistics if they are available.\n",
      " |\n",
      " |      >>> df.explain(\"cost\")\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...Statistics...\n",
      " |      ...\n",
      " |\n",
      " |  fillna(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |\n",
      " |      .. versionadded:: 1.3.1\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : int, float, string, bool or dict\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, float, boolean, or string.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data types are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with replaced null values.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (10, 80.5, \"Alice\", None),\n",
      " |      ...     (5, None, \"Bob\", None),\n",
      " |      ...     (None, None, \"Tom\", None),\n",
      " |      ...     (None, None, None, True)],\n",
      " |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
      " |\n",
      " |      Fill all null values with 50 for numeric columns.\n",
      " |\n",
      " |      >>> df.na.fill(50).show()\n",
      " |      +---+------+-----+----+\n",
      " |      |age|height| name|bool|\n",
      " |      +---+------+-----+----+\n",
      " |      | 10|  80.5|Alice|null|\n",
      " |      |  5|  50.0|  Bob|null|\n",
      " |      | 50|  50.0|  Tom|null|\n",
      " |      | 50|  50.0| null|true|\n",
      " |      +---+------+-----+----+\n",
      " |\n",
      " |      Fill all null values with ``False`` for boolean columns.\n",
      " |\n",
      " |      >>> df.na.fill(False).show()\n",
      " |      +----+------+-----+-----+\n",
      " |      | age|height| name| bool|\n",
      " |      +----+------+-----+-----+\n",
      " |      |  10|  80.5|Alice|false|\n",
      " |      |   5|  null|  Bob|false|\n",
      " |      |null|  null|  Tom|false|\n",
      " |      |null|  null| null| true|\n",
      " |      +----+------+-----+-----+\n",
      " |\n",
      " |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
      " |\n",
      " |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+----+\n",
      " |      |age|height|   name|bool|\n",
      " |      +---+------+-------+----+\n",
      " |      | 10|  80.5|  Alice|null|\n",
      " |      |  5|  null|    Bob|null|\n",
      " |      | 50|  null|    Tom|null|\n",
      " |      | 50|  null|unknown|true|\n",
      " |      +---+------+-------+----+\n",
      " |\n",
      " |  filter(self, condition: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Filters rows using the given condition.\n",
      " |\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column` or str\n",
      " |          a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expressions.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Filtered DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |\n",
      " |      Filter by :class:`Column` instances.\n",
      " |\n",
      " |      >>> df.filter(df.age > 3).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.where(df.age == 2).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |\n",
      " |      Filter by SQL expression in a string.\n",
      " |\n",
      " |      >>> df.filter(\"age > 3\").show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.where(\"age = 2\").show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |\n",
      " |  first(self) -> Optional[pyspark.sql.types.Row]\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Row`\n",
      " |          First row if :class:`DataFrame` is not empty, otherwise ``None``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |\n",
      " |  foreach(self, f: Callable[[pyspark.sql.types.Row], NoneType]) -> None\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |\n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          A function that accepts one parameter which will\n",
      " |          receive each row to process.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> def func(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(func)\n",
      " |\n",
      " |  foreachPartition(self, f: Callable[[Iterator[pyspark.sql.types.Row]], NoneType]) -> None\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |\n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          A function that accepts one parameter which will receive\n",
      " |          each partition to process.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> def func(itr):\n",
      " |      ...     for person in itr:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(func)\n",
      " |\n",
      " |  freqItems(self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None) -> 'DataFrame'\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list or tuple\n",
      " |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      support : float, optional\n",
      " |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with frequent items.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      " |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
      " |      +------------+------------+\n",
      " |      |c1_freqItems|c2_freqItems|\n",
      " |      +------------+------------+\n",
      " |      |   [4, 1, 3]| [8, 11, 10]|\n",
      " |      +------------+------------+\n",
      " |\n",
      " |  groupBy(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      " |          or list of them.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`GroupedData`\n",
      " |          Grouped data by given columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |\n",
      " |      Empty grouping columns triggers a global aggregation.\n",
      " |\n",
      " |      >>> df.groupBy().avg().show()\n",
      " |      +--------+\n",
      " |      |avg(age)|\n",
      " |      +--------+\n",
      " |      |    2.75|\n",
      " |      +--------+\n",
      " |\n",
      " |      Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n",
      " |\n",
      " |      >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n",
      " |      +-----+--------+\n",
      " |      | name|sum(age)|\n",
      " |      +-----+--------+\n",
      " |      |Alice|       2|\n",
      " |      |  Bob|       9|\n",
      " |      +-----+--------+\n",
      " |\n",
      " |      Group-by 'name', and calculate maximum values.\n",
      " |\n",
      " |      >>> df.groupBy(df.name).max().sort(\"name\").show()\n",
      " |      +-----+--------+\n",
      " |      | name|max(age)|\n",
      " |      +-----+--------+\n",
      " |      |Alice|       2|\n",
      " |      |  Bob|       5|\n",
      " |      +-----+--------+\n",
      " |\n",
      " |      Group-by 'name' and 'age', and calculate the number of rows in each group.\n",
      " |\n",
      " |      >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n",
      " |      +-----+---+-----+\n",
      " |      | name|age|count|\n",
      " |      +-----+---+-----+\n",
      " |      |Alice|  2|    1|\n",
      " |      |  Bob|  2|    2|\n",
      " |      |  Bob|  5|    1|\n",
      " |      +-----+---+-----+\n",
      " |\n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |\n",
      " |      .. versionadded:: 1.4\n",
      " |\n",
      " |  head(self, n: Optional[int] = None) -> Union[pyspark.sql.types.Row, NoneType, List[pyspark.sql.types.Row]]\n",
      " |      Returns the first ``n`` rows.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          default 1. Number of rows to return.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      If n is greater than 1, return a list of :class:`Row`.\n",
      " |      If n is 1, return a single Row.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |\n",
      " |  hint(self, name: str, *parameters: Union[ForwardRef('PrimitiveType'), List[ForwardRef('PrimitiveType')]]) -> 'DataFrame'\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 2.2.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          A name of the hint.\n",
      " |      parameters : str, list, float or int\n",
      " |          Optional parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Hinted DataFrame\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      " |      >>> df.join(df2, \"name\").explain()  # doctest: +SKIP\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      ... +- SortMergeJoin ...\n",
      " |      ...\n",
      " |\n",
      " |      Explicitly trigger the broadcast hashjoin by providing the hint in ``df2``.\n",
      " |\n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").explain()\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      ... +- BroadcastHashJoin ...\n",
      " |      ...\n",
      " |\n",
      " |  inputFiles(self) -> List[str]\n",
      " |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      " |      This method simply asks each constituent BaseRelation for its respective files and\n",
      " |      takes the union of all results. Depending on the source relations, this may not find\n",
      " |      all input files. Duplicates are removed.\n",
      " |\n",
      " |      .. versionadded:: 3.1.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of file paths.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a single-row DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).repartition(1).write.json(d, mode=\"overwrite\")\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     df = spark.read.format(\"json\").load(d)\n",
      " |      ...\n",
      " |      ...     # Returns the number of input files.\n",
      " |      ...     len(df.inputFiles())\n",
      " |      1\n",
      " |\n",
      " |  intersect(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      Note that any duplicates are removed. To preserve duplicates\n",
      " |      use :func:`intersectAll`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be combined.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Combined DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      >>> df1.intersect(df2).sort(df1.C1.desc()).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  b|  3|\n",
      " |      |  a|  1|\n",
      " |      +---+---+\n",
      " |\n",
      " |  intersectAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      " |      and another :class:`DataFrame` while preserving duplicates.\n",
      " |\n",
      " |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      " |      resolves columns by position (not by name).\n",
      " |\n",
      " |      .. versionadded:: 2.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be combined.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Combined DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |\n",
      " |  isEmpty(self) -> bool\n",
      " |      Returns ``True`` if this :class:`DataFrame` is empty.\n",
      " |\n",
      " |      .. versionadded:: 3.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Whether it's empty DataFrame or not.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
      " |      >>> df_non_empty = spark.createDataFrame([\"a\"], 'STRING')\n",
      " |      >>> df_empty.isEmpty()\n",
      " |      True\n",
      " |      >>> df_non_empty.isEmpty()\n",
      " |      False\n",
      " |\n",
      " |  isLocal(self) -> bool\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.sql(\"SHOW TABLES\")\n",
      " |      >>> df.isLocal()\n",
      " |      True\n",
      " |\n",
      " |  join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the join\n",
      " |      on : str, list or :class:`Column`, optional\n",
      " |          a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      how : str, optional\n",
      " |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Joined DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
      " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      " |      >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
      " |      >>> df4 = spark.createDataFrame([\n",
      " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      " |      ...     Row(age=None, height=None, name=None),\n",
      " |      ... ])\n",
      " |\n",
      " |      Inner join on columns (default)\n",
      " |\n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
      " |      +----+------+\n",
      " |      |name|height|\n",
      " |      +----+------+\n",
      " |      | Bob|    85|\n",
      " |      +----+------+\n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
      " |      +----+---+\n",
      " |      |name|age|\n",
      " |      +----+---+\n",
      " |      | Bob|  5|\n",
      " |      +----+---+\n",
      " |\n",
      " |      Outer join for both DataFrames on the 'name' column.\n",
      " |\n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
      " |      ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
      " |      +-----+------+\n",
      " |      | name|height|\n",
      " |      +-----+------+\n",
      " |      |  Bob|    85|\n",
      " |      |Alice|  null|\n",
      " |      | null|    80|\n",
      " |      +-----+------+\n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
      " |      +-----+------+\n",
      " |      | name|height|\n",
      " |      +-----+------+\n",
      " |      |  Tom|    80|\n",
      " |      |  Bob|    85|\n",
      " |      |Alice|  null|\n",
      " |      +-----+------+\n",
      " |\n",
      " |      Outer join for both DataFrams with multiple columns.\n",
      " |\n",
      " |      >>> df.join(\n",
      " |      ...     df3,\n",
      " |      ...     [df.name == df3.name, df.age == df3.age],\n",
      " |      ...     'outer'\n",
      " |      ... ).select(df.name, df3.age).show()\n",
      " |      +-----+---+\n",
      " |      | name|age|\n",
      " |      +-----+---+\n",
      " |      |Alice|  2|\n",
      " |      |  Bob|  5|\n",
      " |      +-----+---+\n",
      " |\n",
      " |  limit(self, num: int) -> 'DataFrame'\n",
      " |      Limits the result count to the number specified.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          Number of records to return. Will return this number of records\n",
      " |          or all records if the DataFrame contains less than this number of records.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Subset of the records\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.limit(1).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      | 14| Tom|\n",
      " |      +---+----+\n",
      " |      >>> df.limit(0).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      +---+----+\n",
      " |\n",
      " |  localCheckpoint(self, eager: bool = True) -> 'DataFrame'\n",
      " |      Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
      " |      used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      " |\n",
      " |      .. versionadded:: 2.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional, default True\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Checkpointed DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.localCheckpoint(False)\n",
      " |      DataFrame[age: bigint, name: string]\n",
      " |\n",
      " |  melt(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
      " |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
      " |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
      " |      except for the aggregation, which cannot be reversed.\n",
      " |\n",
      " |      :func:`melt` is an alias for :func:`unpivot`.\n",
      " |\n",
      " |      .. versionadded:: 3.4.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ids : str, Column, tuple, list, optional\n",
      " |          Column(s) to use as identifiers. Can be a single column or column name,\n",
      " |          or a list or tuple for multiple columns.\n",
      " |      values : str, Column, tuple, list, optional\n",
      " |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
      " |          for multiple columns. If not specified or empty, use all columns that\n",
      " |          are not set as `ids`.\n",
      " |      variableColumnName : str\n",
      " |          Name of the variable column.\n",
      " |      valueColumnName : str\n",
      " |          Name of the value column.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Unpivoted DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.unpivot\n",
      " |\n",
      " |  observe(self, observation: Union[ForwardRef('Observation'), str], *exprs: pyspark.sql.column.Column) -> 'DataFrame'\n",
      " |      Define (named) metrics to observe on the DataFrame. This method returns an 'observed'\n",
      " |      DataFrame that returns the same result as the input, with the following guarantees:\n",
      " |\n",
      " |      * It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
      " |          the Dataset at that point.\n",
      " |\n",
      " |      * It will report the value of the defined aggregate columns as soon as we reach a completion\n",
      " |          point. A completion point is either the end of a query (batch mode) or the end of a\n",
      " |          streaming epoch. The value of the aggregates only reflects the data processed since\n",
      " |          the previous completion point.\n",
      " |\n",
      " |      The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
      " |      more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
      " |      contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
      " |      function.\n",
      " |\n",
      " |      A user can observe these metrics by adding\n",
      " |      Python's :class:`~pyspark.sql.streaming.StreamingQueryListener`,\n",
      " |      Scala/Java's ``org.apache.spark.sql.streaming.StreamingQueryListener`` or Scala/Java's\n",
      " |      ``org.apache.spark.sql.util.QueryExecutionListener`` to the spark session.\n",
      " |\n",
      " |      .. versionadded:: 3.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      observation : :class:`Observation` or str\n",
      " |          `str` to specify the name, or an :class:`Observation` instance to obtain the metric.\n",
      " |\n",
      " |          .. versionchanged:: 3.4.0\n",
      " |             Added support for `str` in this parameter.\n",
      " |      exprs : :class:`Column`\n",
      " |          column expressions (:class:`Column`).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          the observed :class:`DataFrame`.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      When ``observation`` is :class:`Observation`, this method only supports batch queries.\n",
      " |      When ``observation`` is a string, this method works for both batch and streaming queries.\n",
      " |      Continuous execution is currently not supported yet.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      When ``observation`` is :class:`Observation`, only batch queries work as below.\n",
      " |\n",
      " |      >>> from pyspark.sql.functions import col, count, lit, max\n",
      " |      >>> from pyspark.sql import Observation\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> observation = Observation(\"my metrics\")\n",
      " |      >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      " |      >>> observed_df.count()\n",
      " |      2\n",
      " |      >>> observation.get\n",
      " |      {'count': 2, 'max(age)': 5}\n",
      " |\n",
      " |      When ``observation`` is a string, streaming queries also work as below.\n",
      " |\n",
      " |      >>> from pyspark.sql.streaming import StreamingQueryListener\n",
      " |      >>> class MyErrorListener(StreamingQueryListener):\n",
      " |      ...    def onQueryStarted(self, event):\n",
      " |      ...        pass\n",
      " |      ...\n",
      " |      ...    def onQueryProgress(self, event):\n",
      " |      ...        row = event.progress.observedMetrics.get(\"my_event\")\n",
      " |      ...        # Trigger if the number of errors exceeds 5 percent\n",
      " |      ...        num_rows = row.rc\n",
      " |      ...        num_error_rows = row.erc\n",
      " |      ...        ratio = num_error_rows / num_rows\n",
      " |      ...        if ratio > 0.05:\n",
      " |      ...            # Trigger alert\n",
      " |      ...            pass\n",
      " |      ...\n",
      " |      ...    def onQueryTerminated(self, event):\n",
      " |      ...        pass\n",
      " |      ...\n",
      " |      >>> spark.streams.addListener(MyErrorListener())\n",
      " |      >>> # Observe row count (rc) and error row count (erc) in the streaming Dataset\n",
      " |      ... observed_ds = df.observe(\n",
      " |      ...     \"my_event\",\n",
      " |      ...     count(lit(1)).alias(\"rc\"),\n",
      " |      ...     count(col(\"error\")).alias(\"erc\"))  # doctest: +SKIP\n",
      " |      >>> observed_ds.writeStream.format(\"console\").start()  # doctest: +SKIP\n",
      " |\n",
      " |  orderBy = sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |\n",
      " |  pandas_api(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
      " |\n",
      " |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
      " |      to pandas-on-Spark, it will lose the index information and the original index\n",
      " |      will be turned into a normal column.\n",
      " |\n",
      " |      This is only available if Pandas is installed and available.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index_col: str or list of str, optional, default: None\n",
      " |          Index column of table in Spark.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`PandasOnSparkDataFrame`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.pandas.frame.DataFrame.to_spark\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      >>> df.pandas_api()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0   14    Tom\n",
      " |      1   23  Alice\n",
      " |      2   16    Bob\n",
      " |\n",
      " |      We can specify the index columns.\n",
      " |\n",
      " |      >>> df.pandas_api(index_col=\"age\")  # doctest: +SKIP\n",
      " |            name\n",
      " |      age\n",
      " |      14     Tom\n",
      " |      23   Alice\n",
      " |      16     Bob\n",
      " |\n",
      " |  persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      storageLevel : :class:`StorageLevel`\n",
      " |          Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Persisted DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> df.persist()\n",
      " |      DataFrame[id: bigint]\n",
      " |\n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      InMemoryTableScan ...\n",
      " |\n",
      " |      Persists the data in the disk by specifying the storage level.\n",
      " |\n",
      " |      >>> from pyspark.storagelevel import StorageLevel\n",
      " |      >>> df.persist(StorageLevel.DISK_ONLY)\n",
      " |      DataFrame[id: bigint]\n",
      " |\n",
      " |  printSchema(self) -> None\n",
      " |      Prints out the schema in the tree format.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: long (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |\n",
      " |  randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[ForwardRef('DataFrame')]\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weights : list\n",
      " |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      seed : int, optional\n",
      " |          The seed for sampling.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of DataFrames.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      " |      ...     Row(age=None, height=None, name=None),\n",
      " |      ... ])\n",
      " |\n",
      " |      >>> splits = df.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      2\n",
      " |      >>> splits[1].count()\n",
      " |      2\n",
      " |\n",
      " |  registerTempTable(self, name: str) -> None\n",
      " |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
      " |\n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      .. deprecated:: 2.0.0\n",
      " |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the temporary table to register.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |\n",
      " |  repartition(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |\n",
      " |          .. versionchanged:: 1.6.0\n",
      " |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |             optional if partitioning columns are specified.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Repartitioned DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      Repartition the data into 10 partitions.\n",
      " |\n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |\n",
      " |      Repartition the data into 7 partitions by 'age' column.\n",
      " |\n",
      " |      >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
      " |      7\n",
      " |\n",
      " |      Repartition the data into 7 partitions by 'age' and 'name columns.\n",
      " |\n",
      " |      >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
      " |      3\n",
      " |\n",
      " |  repartitionByRange(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |\n",
      " |      .. versionadded:: 2.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Repartitioned DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |\n",
      " |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
      " |      Hence, the output may not be consistent, since sampling can return different values.\n",
      " |      The sample size can be controlled by the config\n",
      " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      Repartition the data into 2 partitions by range in 'age' column.\n",
      " |      For example, the first partition can have ``(14, \"Tom\")``, and the second\n",
      " |      partition would have ``(16, \"Bob\")`` and ``(23, \"Alice\")``.\n",
      " |\n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |\n",
      " |  replace(self, to_replace: Union[ForwardRef('LiteralType'), List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : bool, int, float, string, list or dict\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      value : bool, int, float, string or None, optional\n",
      " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      subset : list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data types are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with replaced values.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (10, 80, \"Alice\"),\n",
      " |      ...     (5, None, \"Bob\"),\n",
      " |      ...     (None, 10, \"Tom\"),\n",
      " |      ...     (None, None, None)],\n",
      " |      ...     schema=[\"age\", \"height\", \"name\"])\n",
      " |\n",
      " |      Replace 10 to 20 in all columns.\n",
      " |\n",
      " |      >>> df.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|    20|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |\n",
      " |      Replace 'Alice' to null in all columns.\n",
      " |\n",
      " |      >>> df.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|    10| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |\n",
      " |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
      " |\n",
      " |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|    10| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |\n",
      " |  rollup(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          Columns to roll-up by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      " |          or list of them.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`GroupedData`\n",
      " |          Rolled-up data by given columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |\n",
      " |  sameSemantics(self, other: 'DataFrame') -> bool\n",
      " |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
      " |      therefore return the same results.\n",
      " |\n",
      " |      .. versionadded:: 3.1.0\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
      " |      such as attribute names.\n",
      " |\n",
      " |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
      " |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      " |      different plans. Such false negative semantic can be useful when caching as an example.\n",
      " |\n",
      " |      This API is a developer API.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          The other DataFrame to compare against.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Whether these two DataFrames are similar.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.range(10)\n",
      " |      >>> df2 = spark.range(10)\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      " |      True\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      " |      False\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      " |      True\n",
      " |\n",
      " |  sample(self, withReplacement: Union[float, bool, NoneType] = None, fraction: Union[int, float, NoneType] = None, seed: Optional[int] = None) -> 'DataFrame'\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool, optional\n",
      " |          Sample with replacement or not (default ``False``).\n",
      " |      fraction : float, optional\n",
      " |          Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      seed : int, optional\n",
      " |          Seed for sampling (default a random seed).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Sampled rows from given DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |      count of the given :class:`DataFrame`.\n",
      " |\n",
      " |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      7\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      7\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |\n",
      " |  sampleBy(self, col: 'ColumnOrName', fractions: Dict[Any, float], seed: Optional[int] = None) -> 'DataFrame'\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |\n",
      " |      .. versionadded:: 1.5.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : :class:`Column` or str\n",
      " |          column that defines strata\n",
      " |\n",
      " |          .. versionchanged:: 3.0.0\n",
      " |             Added sampling by a column of :class:`Column`\n",
      " |      fractions : dict\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      a new :class:`DataFrame` that represents the stratified sample\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    3|\n",
      " |      |  1|    6|\n",
      " |      +---+-----+\n",
      " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      " |      33\n",
      " |\n",
      " |  select(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column`, or list\n",
      " |          column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          A DataFrame with subset (or all) of columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |\n",
      " |      Select all columns in the DataFrame.\n",
      " |\n",
      " |      >>> df.select('*').show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |\n",
      " |      Select a column with other expressions in the DataFrame.\n",
      " |\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
      " |      +-----+---+\n",
      " |      | name|age|\n",
      " |      +-----+---+\n",
      " |      |Alice| 12|\n",
      " |      |  Bob| 15|\n",
      " |      +-----+---+\n",
      " |\n",
      " |  selectExpr(self, *expr: Union[str, List[str]]) -> 'DataFrame'\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |\n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          A DataFrame with new/old columns transformed by expressions.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").show()\n",
      " |      +---------+--------+\n",
      " |      |(age * 2)|abs(age)|\n",
      " |      +---------+--------+\n",
      " |      |        4|       2|\n",
      " |      |       10|       5|\n",
      " |      +---------+--------+\n",
      " |\n",
      " |  semanticHash(self) -> int\n",
      " |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 3.1.0\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike the standard hash code, the hash is calculated against the query plan\n",
      " |      simplified by tolerating the cosmetic differences such as attribute names.\n",
      " |\n",
      " |      This API is a developer API.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Hash value.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |\n",
      " |  show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          Number of rows to show.\n",
      " |      truncate : bool or int, optional\n",
      " |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      vertical : bool, optional\n",
      " |          If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      Show only top 2 rows.\n",
      " |\n",
      " |      >>> df.show(2)\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      | 14|  Tom|\n",
      " |      | 23|Alice|\n",
      " |      +---+-----+\n",
      " |      only showing top 2 rows\n",
      " |\n",
      " |      Show :class:`DataFrame` where the maximum number of characters is 3.\n",
      " |\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      | 14| Tom|\n",
      " |      | 23| Ali|\n",
      " |      | 16| Bob|\n",
      " |      +---+----+\n",
      " |\n",
      " |      Show :class:`DataFrame` vertically.\n",
      " |\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |      age  | 14\n",
      " |      name | Tom\n",
      " |      -RECORD 1-----\n",
      " |      age  | 23\n",
      " |      name | Alice\n",
      " |      -RECORD 2-----\n",
      " |      age  | 16\n",
      " |      name | Bob\n",
      " |\n",
      " |  sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list, or :class:`Column`, optional\n",
      " |           list of :class:`Column` or column names to sort by.\n",
      " |\n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional, default True\n",
      " |          boolean or list of boolean.\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Sorted DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import desc, asc\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |\n",
      " |      Sort the DataFrame in ascending order.\n",
      " |\n",
      " |      >>> df.sort(asc(\"age\")).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |\n",
      " |      Sort the DataFrame in descending order.\n",
      " |\n",
      " |      >>> df.sort(df.age.desc()).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> df.orderBy(df.age.desc()).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> df.sort(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |\n",
      " |      Specify multiple columns\n",
      " |\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|  Bob|\n",
      " |      +---+-----+\n",
      " |\n",
      " |      Specify multiple columns for sorting order at `ascending`.\n",
      " |\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |\n",
      " |  sortWithinPartitions(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |\n",
      " |      .. versionadded:: 1.6.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list or :class:`Column`, optional\n",
      " |          list of :class:`Column` or column names to sort by.\n",
      " |\n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional, default True\n",
      " |          boolean or list of boolean.\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame sorted by partitions.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False)\n",
      " |      DataFrame[age: bigint, name: string]\n",
      " |\n",
      " |  subtract(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be subtracted.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Subtracted DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      >>> df1.subtract(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |\n",
      " |  summary(self, *statistics: str) -> 'DataFrame'\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      " |\n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |\n",
      " |      .. versionadded:: 2.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      statistics : str, optional\n",
      " |           Column names to calculate statistics by (default All columns).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          A new DataFrame that provides statistics for the given DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      " |      ... )\n",
      " |      >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |summary| age|            weight|           height|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |  count|   3|                 3|                3|\n",
      " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      " |      |    min|  11|              37.8|            142.2|\n",
      " |      |    25%|  11|              37.8|            142.2|\n",
      " |      |    50%|  12|              40.3|            142.3|\n",
      " |      |    75%|  13|              44.1|            150.5|\n",
      " |      |    max|  13|              44.1|            150.5|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |\n",
      " |      >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+------+------+\n",
      " |      |summary|age|weight|height|\n",
      " |      +-------+---+------+------+\n",
      " |      |  count|  3|     3|     3|\n",
      " |      |    min| 11|  37.8| 142.2|\n",
      " |      |    25%| 11|  37.8| 142.2|\n",
      " |      |    75%| 13|  44.1| 150.5|\n",
      " |      |    max| 13|  44.1| 150.5|\n",
      " |      +-------+---+------+------+\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.display\n",
      " |\n",
      " |  tail(self, num: int) -> List[pyspark.sql.types.Row]\n",
      " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |\n",
      " |      Running tail requires moving data into the application's driver process, and doing so with\n",
      " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      " |\n",
      " |      .. versionadded:: 3.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          Number of records to return. Will return this number of records\n",
      " |          or all records if the DataFrame contains less than this number of records.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of rows\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      >>> df.tail(2)\n",
      " |      [Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      " |\n",
      " |  take(self, num: int) -> List[pyspark.sql.types.Row]\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          Number of records to return. Will return this number of records\n",
      " |          or all records if the DataFrame contains less than this number of records..\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of rows\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      Return the first 2 rows of the :class:`DataFrame`.\n",
      " |\n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\n",
      " |\n",
      " |  to(self, schema: pyspark.sql.types.StructType) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` where each row is reconciled to match the specified\n",
      " |      schema.\n",
      " |\n",
      " |      .. versionadded:: 3.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`StructType`\n",
      " |          Specified schema.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Reconciled DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      * Reorder columns and/or inner fields by name to match the specified schema.\n",
      " |\n",
      " |      * Project away columns and/or inner fields that are not needed by the specified schema.\n",
      " |          Missing columns and/or inner fields (present in the specified schema but not input\n",
      " |          DataFrame) lead to failures.\n",
      " |\n",
      " |      * Cast the columns and/or inner fields to match the data types in the specified schema,\n",
      " |          if the types are compatible, e.g., numeric to numeric (error if overflows), but\n",
      " |          not string to int.\n",
      " |\n",
      " |      * Carry over the metadata from the specified schema, while the columns and/or inner fields\n",
      " |          still keep their own metadata if not overwritten by the specified schema.\n",
      " |\n",
      " |      * Fail if the nullability is not compatible. For example, the column and/or inner field\n",
      " |          is nullable but the specified schema requires them to be not nullable.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.types import StructField, StringType\n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1)], [\"i\", \"j\"])\n",
      " |      >>> df.schema\n",
      " |      StructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])\n",
      " |\n",
      " |      >>> schema = StructType([StructField(\"j\", StringType()), StructField(\"i\", StringType())])\n",
      " |      >>> df2 = df.to(schema)\n",
      " |      >>> df2.schema\n",
      " |      StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])\n",
      " |      >>> df2.show()\n",
      " |      +---+---+\n",
      " |      |  j|  i|\n",
      " |      +---+---+\n",
      " |      |  1|  a|\n",
      " |      +---+---+\n",
      " |\n",
      " |  toDF(self, *cols: str) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` that with new specified column names\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *cols : tuple\n",
      " |          a tuple of string new column name. The length of the\n",
      " |          list needs to be the same as the number of columns in the initial\n",
      " |          :class:`DataFrame`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with new column names.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"),\n",
      " |      ...     (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.toDF('f1', 'f2').show()\n",
      " |      +---+-----+\n",
      " |      | f1|   f2|\n",
      " |      +---+-----+\n",
      " |      | 14|  Tom|\n",
      " |      | 23|Alice|\n",
      " |      | 16|  Bob|\n",
      " |      +---+-----+\n",
      " |\n",
      " |  toJSON(self, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |\n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      use_unicode : bool, optional, default True\n",
      " |          Whether to convert to unicode or not.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |\n",
      " |  toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[pyspark.sql.types.Row]\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      " |      partitions.\n",
      " |\n",
      " |      .. versionadded:: 2.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefetchPartitions : bool, optional\n",
      " |          If Spark should pre-fetch the next partition before it is needed.\n",
      " |\n",
      " |          .. versionchanged:: 3.4.0\n",
      " |              This argument does not take effect for Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Iterator\n",
      " |          Iterator of rows.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      " |\n",
      " |  to_koalas(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      # Keep to_koalas for backward compatibility for now.\n",
      " |\n",
      " |  to_pandas_on_spark(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      # Keep to_pandas_on_spark for backward compatibility for now.\n",
      " |\n",
      " |  transform(self, func: Callable[..., ForwardRef('DataFrame')], *args: Any, **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      " |\n",
      " |      .. versionadded:: 3.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a function that takes and returns a :class:`DataFrame`.\n",
      " |      *args\n",
      " |          Positional arguments to pass to func.\n",
      " |\n",
      " |          .. versionadded:: 3.3.0\n",
      " |      **kwargs\n",
      " |          Keyword arguments to pass to func.\n",
      " |\n",
      " |          .. versionadded:: 3.3.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Transformed DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      " |      >>> def cast_all_to_int(input_df):\n",
      " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      " |      >>> def sort_columns_asc(input_df):\n",
      " |      ...     return input_df.select(*sorted(input_df.columns))\n",
      " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      " |      +-----+---+\n",
      " |      |float|int|\n",
      " |      +-----+---+\n",
      " |      |    1|  1|\n",
      " |      |    2|  2|\n",
      " |      +-----+---+\n",
      " |\n",
      " |      >>> def add_n(input_df, n):\n",
      " |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
      " |      ...                             for col_name in input_df.columns])\n",
      " |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
      " |      +---+-----+\n",
      " |      |int|float|\n",
      " |      +---+-----+\n",
      " |      | 12| 12.0|\n",
      " |      | 13| 13.0|\n",
      " |      +---+-----+\n",
      " |\n",
      " |  union(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 2.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be unioned\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.unionAll\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |\n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.union(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   4|   5|   6|\n",
      " |      +----+----+----+\n",
      " |      >>> df1.union(df1).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   1|   2|   3|\n",
      " |      +----+----+----+\n",
      " |\n",
      " |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be combined\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Combined DataFrame\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |\n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |\n",
      " |      :func:`unionAll` is an alias to :func:`union`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.union\n",
      " |\n",
      " |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |\n",
      " |      .. versionadded:: 2.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be combined.\n",
      " |      allowMissingColumns : bool, optional, default False\n",
      " |         Specify whether to allow missing columns.\n",
      " |\n",
      " |         .. versionadded:: 3.1.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Combined DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |\n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |\n",
      " |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n",
      " |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n",
      " |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n",
      " |      in the schema of the union result:\n",
      " |\n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      " |      +----+----+----+----+\n",
      " |      |col0|col1|col2|col3|\n",
      " |      +----+----+----+----+\n",
      " |      |   1|   2|   3|null|\n",
      " |      |null|   4|   5|   6|\n",
      " |      +----+----+----+----+\n",
      " |\n",
      " |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blocking : bool\n",
      " |          Whether to block until all blocks are deleted.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Unpersisted DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> df.persist()\n",
      " |      DataFrame[id: bigint]\n",
      " |      >>> df.unpersist()\n",
      " |      DataFrame[id: bigint]\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> df.unpersist(True)\n",
      " |      DataFrame[id: bigint]\n",
      " |\n",
      " |  unpivot(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
      " |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
      " |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
      " |      except for the aggregation, which cannot be reversed.\n",
      " |\n",
      " |      This function is useful to massage a DataFrame into a format where some\n",
      " |      columns are identifier columns (\"ids\"), while all other columns (\"values\")\n",
      " |      are \"unpivoted\" to the rows, leaving just two non-id columns, named as given\n",
      " |      by `variableColumnName` and `valueColumnName`.\n",
      " |\n",
      " |      When no \"id\" columns are given, the unpivoted DataFrame consists of only the\n",
      " |      \"variable\" and \"value\" columns.\n",
      " |\n",
      " |      The `values` columns must not be empty so at least one value must be given to be unpivoted.\n",
      " |      When `values` is `None`, all non-id columns will be unpivoted.\n",
      " |\n",
      " |      All \"value\" columns must share a least common data type. Unless they are the same data type,\n",
      " |      all \"value\" columns are cast to the nearest common data type. For instance, types\n",
      " |      `IntegerType` and `LongType` are cast to `LongType`, while `IntegerType` and `StringType`\n",
      " |      do not have a common data type and `unpivot` fails.\n",
      " |\n",
      " |      .. versionadded:: 3.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ids : str, Column, tuple, list\n",
      " |          Column(s) to use as identifiers. Can be a single column or column name,\n",
      " |          or a list or tuple for multiple columns.\n",
      " |      values : str, Column, tuple, list, optional\n",
      " |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
      " |          for multiple columns. If specified, must not be empty. If not specified, uses all\n",
      " |          columns that are not set as `ids`.\n",
      " |      variableColumnName : str\n",
      " |          Name of the variable column.\n",
      " |      valueColumnName : str\n",
      " |          Name of the value column.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Unpivoted DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(1, 11, 1.1), (2, 12, 1.2)],\n",
      " |      ...     [\"id\", \"int\", \"double\"],\n",
      " |      ... )\n",
      " |      >>> df.show()\n",
      " |      +---+---+------+\n",
      " |      | id|int|double|\n",
      " |      +---+---+------+\n",
      " |      |  1| 11|   1.1|\n",
      " |      |  2| 12|   1.2|\n",
      " |      +---+---+------+\n",
      " |\n",
      " |      >>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n",
      " |      +---+------+----+\n",
      " |      | id|   var| val|\n",
      " |      +---+------+----+\n",
      " |      |  1|   int|11.0|\n",
      " |      |  1|double| 1.1|\n",
      " |      |  2|   int|12.0|\n",
      " |      |  2|double| 1.2|\n",
      " |      +---+------+----+\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.melt\n",
      " |\n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |\n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, name of the new column.\n",
      " |      col : :class:`Column`\n",
      " |          a :class:`Column` expression for the new column.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with new or replaced column.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method introduces a projection internally. Therefore, calling it multiple\n",
      " |      times, for instance, via loops in order to add multiple columns can generate big\n",
      " |      plans which can cause performance issues and even `StackOverflowException`.\n",
      " |      To avoid this, use :func:`select` with multiple columns at once.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.withColumn('age2', df.age + 2).show()\n",
      " |      +---+-----+----+\n",
      " |      |age| name|age2|\n",
      " |      +---+-----+----+\n",
      " |      |  2|Alice|   4|\n",
      " |      |  5|  Bob|   7|\n",
      " |      +---+-----+----+\n",
      " |\n",
      " |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if the schema doesn't contain the given column name.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      existing : str\n",
      " |          string, name of the existing column to rename.\n",
      " |      new : str\n",
      " |          string, new name of the column.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with renamed column.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.withColumnRenamed('age', 'age2').show()\n",
      " |      +----+-----+\n",
      " |      |age2| name|\n",
      " |      +----+-----+\n",
      " |      |   2|Alice|\n",
      " |      |   5|  Bob|\n",
      " |      +----+-----+\n",
      " |\n",
      " |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
      " |      existing columns that have the same names.\n",
      " |\n",
      " |      The colsMap is a map of column name and column, the column must only refer to attributes\n",
      " |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
      " |\n",
      " |      .. versionadded:: 3.3.0\n",
      " |         Added support for multiple columns adding\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colsMap : dict\n",
      " |          a dict of column name and :class:`Column`. Currently, only a single map is supported.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with new or replaced columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n",
      " |      +---+-----+----+----+\n",
      " |      |age| name|age2|age3|\n",
      " |      +---+-----+----+----+\n",
      " |      |  2|Alice|   4|   5|\n",
      " |      |  5|  Bob|   7|   8|\n",
      " |      +---+-----+----+----+\n",
      " |\n",
      " |  withColumnsRenamed(self, colsMap: Dict[str, str]) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by renaming multiple columns.\n",
      " |      This is a no-op if the schema doesn't contain the given column names.\n",
      " |\n",
      " |      .. versionadded:: 3.4.0\n",
      " |         Added support for multiple columns renaming\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colsMap : dict\n",
      " |          a dict of existing column names and corresponding desired column names.\n",
      " |          Currently, only a single map is supported.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with renamed columns.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`withColumnRenamed`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df = df.withColumns({'age2': df.age + 2, 'age3': df.age + 3})\n",
      " |      >>> df.withColumnsRenamed({'age2': 'age4', 'age3': 'age5'}).show()\n",
      " |      +---+-----+----+----+\n",
      " |      |age| name|age4|age5|\n",
      " |      +---+-----+----+----+\n",
      " |      |  2|Alice|   4|   5|\n",
      " |      |  5|  Bob|   7|   8|\n",
      " |      +---+-----+----+----+\n",
      " |\n",
      " |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
      " |\n",
      " |      .. versionadded:: 3.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      columnName : str\n",
      " |          string, name of the existing column to update the metadata.\n",
      " |      metadata : dict\n",
      " |          dict, new metadata to be assigned to df.schema[columnName].metadata\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with updated metadata column.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
      " |      >>> df_meta.schema['age'].metadata\n",
      " |      {'foo': 'bar'}\n",
      " |\n",
      " |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |\n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |\n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |\n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |\n",
      " |      .. versionadded:: 2.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eventTime : str\n",
      " |          the name of the column that contains the event time of the row.\n",
      " |      delayThreshold : str\n",
      " |          the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Watermarked DataFrame\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is a feature only for Structured Streaming.\n",
      " |\n",
      " |      This API is evolving.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import timestamp_seconds\n",
      " |      >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
      " |      ...     \"value % 5 AS value\", \"timestamp\")\n",
      " |      >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
      " |      DataFrame[value: bigint, time: timestamp]\n",
      " |\n",
      " |      Group the data by window and value (0 - 4), and compute the count of each group.\n",
      " |\n",
      " |      >>> import time\n",
      " |      >>> from pyspark.sql.functions import window\n",
      " |      >>> query = (df\n",
      " |      ...     .withWatermark(\"timestamp\", \"10 minutes\")\n",
      " |      ...     .groupBy(\n",
      " |      ...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n",
      " |      ...         df.value)\n",
      " |      ...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n",
      " |      >>> time.sleep(3)\n",
      " |      >>> query.stop()\n",
      " |\n",
      " |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n",
      " |      Create a write configuration builder for v2 sources.\n",
      " |\n",
      " |      This builder is used to configure and execute write operations.\n",
      " |\n",
      " |      For example, to append or create or replace existing tables.\n",
      " |\n",
      " |      .. versionadded:: 3.1.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      table : str\n",
      " |          Target table name to write to.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameWriterV2`\n",
      " |          DataFrameWriterV2 to use further to specify how to save the data\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      " |      >>> df.writeTo(                              # doctest: +SKIP\n",
      " |      ...     \"catalog.db.table\"\n",
      " |      ... ).partitionedBy(\"col\").createOrReplace()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of column names.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |\n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of columns as tuple pairs.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |\n",
      " |  isStreaming\n",
      " |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n",
      " |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n",
      " |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n",
      " |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n",
      " |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n",
      " |      is a streaming source present.\n",
      " |\n",
      " |      .. versionadded:: 2.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Whether it's streaming DataFrame or not.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.readStream.format(\"rate\").load()\n",
      " |      >>> df.isStreaming\n",
      " |      True\n",
      " |\n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |\n",
      " |      .. versionadded:: 1.3.1\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameNaFunctions`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n",
      " |      >>> type(df.na)\n",
      " |      <class '...dataframe.DataFrameNaFunctions'>\n",
      " |\n",
      " |      Replace the missing values as 2.\n",
      " |\n",
      " |      >>> df.na.fill(2).show()\n",
      " |      +---+---+\n",
      " |      | c1| c2|\n",
      " |      +---+---+\n",
      " |      |  1|  2|\n",
      " |      +---+---+\n",
      " |\n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> type(df.rdd)\n",
      " |      <class 'pyspark.rdd.RDD'>\n",
      " |\n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |\n",
      " |      Retrieve the schema of the current DataFrame.\n",
      " |\n",
      " |      >>> df.schema\n",
      " |      StructType([StructField('age', LongType(), True),\n",
      " |                  StructField('name', StringType(), True)])\n",
      " |\n",
      " |  sparkSession\n",
      " |      Returns Spark session that created this :class:`DataFrame`.\n",
      " |\n",
      " |      .. versionadded:: 3.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`SparkSession`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> type(df.sparkSession)\n",
      " |      <class '...session.SparkSession'>\n",
      " |\n",
      " |  sql_ctx\n",
      " |\n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameStatFunctions`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pyspark.sql.functions as f\n",
      " |      >>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n",
      " |      >>> type(df.stat)\n",
      " |      <class '...dataframe.DataFrameStatFunctions'>\n",
      " |      >>> df.stat.corr(\"id\", \"c\")\n",
      " |      1.0\n",
      " |\n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |\n",
      " |      .. versionadded:: 2.1.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StorageLevel`\n",
      " |          Currently defined storage level.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.range(10)\n",
      " |      >>> df1.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df1.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |\n",
      " |      >>> df2 = spark.range(5)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |\n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |\n",
      " |      .. versionadded:: 1.4.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameWriter`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> type(df.write)\n",
      " |      <class '...readwriter.DataFrameWriter'>\n",
      " |\n",
      " |      Write the DataFrame as a table.\n",
      " |\n",
      " |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
      " |      >>> df.write.saveAsTable(\"tab2\")\n",
      " |      >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
      " |\n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |\n",
      " |      .. versionadded:: 2.0.0\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataStreamWriter`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import time\n",
      " |      >>> import tempfile\n",
      " |      >>> df = spark.readStream.format(\"rate\").load()\n",
      " |      >>> type(df.writeStream)\n",
      " |      <class 'pyspark.sql.streaming.readwriter.DataStreamWriter'>\n",
      " |\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Create a table with Rate source.\n",
      " |      ...     query = df.writeStream.toTable(\n",
      " |      ...         \"my_table\", checkpointLocation=d)\n",
      " |      ...     time.sleep(3)\n",
      " |      ...     query.stop()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |\n",
      " |  mapInArrow(self, func: 'ArrowMapIterFunction', schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrame'\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      The function should take an iterator of `pyarrow.RecordBatch`\\s and return\n",
      " |      another iterator of `pyarrow.RecordBatch`\\s. All columns are passed\n",
      " |      together as an iterator of `pyarrow.RecordBatch`\\s to the function and the\n",
      " |      returned iterator of `pyarrow.RecordBatch`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pyarrow.RecordBatch` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
      " |      output can be different.\n",
      " |\n",
      " |      .. versionadded:: 3.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes an iterator of `pyarrow.RecordBatch`\\s, and\n",
      " |          outputs an iterator of `pyarrow.RecordBatch`\\s.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pyarrow  # doctest: +SKIP\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for batch in iterator:\n",
      " |      ...         pdf = batch.to_pandas()\n",
      " |      ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
      " |      >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is unstable, and for developers.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |      pyspark.sql.DataFrame.mapInPandas\n",
      " |\n",
      " |  mapInPandas(self, func: 'PandasMapIterFunction', schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrame'\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |\n",
      " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pandas.DataFrame` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
      " |      output can be different.\n",
      " |\n",
      " |      .. versionadded:: 3.0.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import pandas_udf\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for pdf in iterator:\n",
      " |      ...         yield pdf[pdf.id == 1]\n",
      " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      " |\n",
      " |  toPandas(self) -> 'PandasDataFrameLike'\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |\n",
      " |      This is only available if Pandas is installed and available.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
      " |      expected to be small, as all the data is loaded into the driver's memory.\n",
      " |\n",
      " |      Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hflight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7426371-ad42-483f-8d0f-4dee075c5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "hflight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e193c46b-ff98-426b-9c99-7d657baafc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------------+------------------+\n",
      "|Origin|Dest|    출발 지연 평균|    도착 지연 평균|\n",
      "+------+----+------------------+------------------+\n",
      "|   HOU| PHL|23.541666666666668| 33.19124423963134|\n",
      "|   IAH| LIT| 6.764063811922754| 7.740771812080537|\n",
      "|   IAH| GSP| 9.617359413202934|  9.08272506082725|\n",
      "|   IAH| DEN| 7.441828254847645| 10.37734288864388|\n",
      "|   IAH| TUS| 7.801679586563307| 7.783870967741936|\n",
      "|   IAH| ABQ| 7.928692699490663| 6.703891708967851|\n",
      "|   HOU| LIT| 3.533333333333333|7.4404432132963985|\n",
      "|   IAH| ANC|26.080645161290324|            24.952|\n",
      "|   IAH| SMF|  4.66271018793274| 9.093966369930762|\n",
      "|   HOU| TUL| 7.855238095238096|11.208174904942966|\n",
      "|   HOU| MDW|3.8126815101645692|13.204150579150578|\n",
      "|   HOU| OKC| 9.441897233201582|12.509865824782953|\n",
      "|   IAH| DSM|15.951104100946372|12.796850393700787|\n",
      "|   IAH| DFW| 5.510741008930727| 8.764620938628159|\n",
      "|   IAH| BOS| 6.559045956951716|10.519141531322505|\n",
      "|   HOU| MAF| 6.918367346938775|10.507042253521126|\n",
      "|   IAH| RIC|12.301231802911534|11.040313549832026|\n",
      "|   HOU| BNA| 5.394160583941606|11.395331874544128|\n",
      "|   IAH| CLE|1.0408067542213884| 5.868198874296435|\n",
      "|   HOU| MSY| 7.040394973070018|10.635058330840563|\n",
      "+------+----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, stddev, col\n",
    "\n",
    "hflight = hflight.withColumn('ArrDelay', col('ArrDelay').cast('integer')).\\\n",
    "            withColumn('DepDelay', col('DepDelay').cast('integer'))\n",
    "\n",
    "#hflight.groupby(\"Origin\", \"Dest\").mean(\"ArrDelay\", \"DepDelay\").show()\n",
    "hflight.groupBy(\"Origin\", \"Dest\").agg(\n",
    "    avg(\"ArrDelay\").alias(\"출발 지연 평균\"),\n",
    "    avg(\"DepDelay\").alias(\"도착 지연 평균\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294613d1-5f47-49ad-98f1-afa1aac0c179",
   "metadata": {},
   "source": [
    "# 문제 2\n",
    "#### 목적지 공항에 대해 연착 건수를 구하고,\n",
    "#### 연착 건수가 2000회 이상인 공항에 대한 데이터만 추출\n",
    "#### col -> Dest : 목적지 공항 ArrDelay(연착은 5분이상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fbf6f60-a32d-4825-8ee5-f32d1fceb8ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sort in module pyspark.sql.dataframe:\n",
      "\n",
      "sort(*cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "\n",
      "    .. versionadded:: 1.3.0\n",
      "\n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    cols : str, list, or :class:`Column`, optional\n",
      "         list of :class:`Column` or column names to sort by.\n",
      "\n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    ascending : bool or list, optional, default True\n",
      "        boolean or list of boolean.\n",
      "        Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "        If a list is specified, the length of the list must equal the length of the `cols`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        Sorted DataFrame.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql.functions import desc, asc\n",
      "    >>> df = spark.createDataFrame([\n",
      "    ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "\n",
      "    Sort the DataFrame in ascending order.\n",
      "\n",
      "    >>> df.sort(asc(\"age\")).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  2|Alice|\n",
      "    |  5|  Bob|\n",
      "    +---+-----+\n",
      "\n",
      "    Sort the DataFrame in descending order.\n",
      "\n",
      "    >>> df.sort(df.age.desc()).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  5|  Bob|\n",
      "    |  2|Alice|\n",
      "    +---+-----+\n",
      "    >>> df.orderBy(df.age.desc()).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  5|  Bob|\n",
      "    |  2|Alice|\n",
      "    +---+-----+\n",
      "    >>> df.sort(\"age\", ascending=False).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  5|  Bob|\n",
      "    |  2|Alice|\n",
      "    +---+-----+\n",
      "\n",
      "    Specify multiple columns\n",
      "\n",
      "    >>> df = spark.createDataFrame([\n",
      "    ...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "    >>> df.orderBy(desc(\"age\"), \"name\").show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  5|  Bob|\n",
      "    |  2|Alice|\n",
      "    |  2|  Bob|\n",
      "    +---+-----+\n",
      "\n",
      "    Specify multiple columns for sorting order at `ascending`.\n",
      "\n",
      "    >>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  5|  Bob|\n",
      "    |  2|  Bob|\n",
      "    |  2|Alice|\n",
      "    +---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hflight.sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7040017-497e-40f3-ad86-e54961a04b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|Dest| Cnt|\n",
      "+----+----+\n",
      "| DAL|3160|\n",
      "| ATL|2515|\n",
      "| LAX|2492|\n",
      "| MSY|2201|\n",
      "| DEN|2177|\n",
      "| ORD|2118|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql.functions import expr\\nhflight.withColumn(\"ArrDelay\", expr(\"ArrDelay == 0\")).show()\\nhflight.where(expr(\"ArrDelay == 0\")).show()\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "hflight= hflight.withColumn('ArrDelay', col('ArrDelay').cast('integer'))\n",
    "hflight.filter(col('ArrDelay') > 5).groupby(\"Dest\").agg(count(col(\"ArrDelay\")).alias(\"Cnt\")).filter(col(\"Cnt\") > 2000).sort(\"Cnt\", ascending=False).show()\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import expr\n",
    "hflight.withColumn(\"ArrDelay\", expr(\"ArrDelay == 0\")).show()\n",
    "hflight.where(expr(\"ArrDelay == 0\")).show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "507e5717-d604-4d52-a585-6aa2b451170e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|ArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|\n",
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|2011|    1|         1|        6|   1400|   1500|           AA|      428| N576AA|               60|     40|   false|       0|   IAH| DFW|     224|     7|     13|        0|            null|       0|\n",
      "|2011|    1|         2|        7|   1401|   1501|           AA|      428| N557AA|               60|     45|   false|       1|   IAH| DFW|     224|     6|      9|        0|            null|       0|\n",
      "|2011|    1|         3|        1|   1352|   1502|           AA|      428| N541AA|               70|     48|   false|      -8|   IAH| DFW|     224|     5|     17|        0|            null|       0|\n",
      "|2011|    1|         4|        2|   1403|   1513|           AA|      428| N403AA|               70|     39|   false|       3|   IAH| DFW|     224|     9|     22|        0|            null|       0|\n",
      "|2011|    1|         5|        3|   1405|   1507|           AA|      428| N492AA|               62|     44|   false|       5|   IAH| DFW|     224|     9|      9|        0|            null|       0|\n",
      "|2011|    1|         6|        4|   1359|   1503|           AA|      428| N262AA|               64|     45|   false|      -1|   IAH| DFW|     224|     6|     13|        0|            null|       0|\n",
      "|2011|    1|         7|        5|   1359|   1509|           AA|      428| N493AA|               70|     43|   false|      -1|   IAH| DFW|     224|    12|     15|        0|            null|       0|\n",
      "|2011|    1|         8|        6|   1355|   1454|           AA|      428| N477AA|               59|     40|   false|      -5|   IAH| DFW|     224|     7|     12|        0|            null|       0|\n",
      "|2011|    1|         9|        7|   1443|   1554|           AA|      428| N476AA|               71|     41|   false|      43|   IAH| DFW|     224|     8|     22|        0|            null|       0|\n",
      "|2011|    1|        10|        1|   1443|   1553|           AA|      428| N504AA|               70|     45|   false|      43|   IAH| DFW|     224|     6|     19|        0|            null|       0|\n",
      "|2011|    1|        11|        2|   1429|   1539|           AA|      428| N565AA|               70|     42|   false|      29|   IAH| DFW|     224|     8|     20|        0|            null|       0|\n",
      "|2011|    1|        12|        3|   1419|   1515|           AA|      428| N577AA|               56|     41|   false|      19|   IAH| DFW|     224|     4|     11|        0|            null|       0|\n",
      "|2011|    1|        13|        4|   1358|   1501|           AA|      428| N476AA|               63|     44|   false|      -2|   IAH| DFW|     224|     6|     13|        0|            null|       0|\n",
      "|2011|    1|        14|        5|   1357|   1504|           AA|      428| N552AA|               67|     47|   false|      -3|   IAH| DFW|     224|     5|     15|        0|            null|       0|\n",
      "|2011|    1|        15|        6|   1359|   1459|           AA|      428| N462AA|               60|     44|   false|      -1|   IAH| DFW|     224|     6|     10|        0|            null|       0|\n",
      "|2011|    1|        16|        7|   1359|   1509|           AA|      428| N555AA|               70|     41|   false|      -1|   IAH| DFW|     224|    12|     17|        0|            null|       0|\n",
      "|2011|    1|        17|        1|   1530|   1634|           AA|      428| N518AA|               64|     48|   false|      90|   IAH| DFW|     224|     8|      8|        0|            null|       0|\n",
      "|2011|    1|        18|        2|   1408|   1508|           AA|      428| N507AA|               60|     42|   false|       8|   IAH| DFW|     224|     7|     11|        0|            null|       0|\n",
      "|2011|    1|        19|        3|   1356|   1503|           AA|      428| N523AA|               67|     46|   false|      -4|   IAH| DFW|     224|    10|     11|        0|            null|       0|\n",
      "|2011|    1|        20|        4|   1507|   1622|           AA|      428| N425AA|               75|     42|   false|      67|   IAH| DFW|     224|     9|     24|        0|            null|       0|\n",
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|ArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|\n",
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|2011|    1|        26|        3|   1634|   1745|           AA|     1121| N556AA|               71|     41|       0|       4|   IAH| DFW|     224|     7|     23|        0|            null|       0|\n",
      "|2011|    1|        15|        6|   1022|   1340|           AA|     1700| N3DEAA|              138|    114|       0|       2|   IAH| MIA|     964|    13|     11|        0|            null|       0|\n",
      "|2011|    1|         6|        4|   1204|   1310|           AA|     1820| N563AA|               66|     41|       0|      -1|   IAH| DFW|     224|     8|     17|        0|            null|       0|\n",
      "|2011|    1|        11|        2|   1205|   1310|           AA|     1820| N563AA|               65|     42|       0|       0|   IAH| DFW|     224|     9|     14|        0|            null|       0|\n",
      "|2011|    1|        27|        4|    606|    915|           AA|     1994| N3DEAA|              129|    112|       0|       6|   IAH| MIA|     964|     4|     13|        0|            null|       0|\n",
      "|2011|    1|        27|        4|   1832|   2110|           AS|      731| N627AS|              278|    262|       0|       7|   IAH| SEA|    1874|     5|     11|        0|            null|       0|\n",
      "|2011|    1|         7|        5|    654|   1117|           B6|      620| N641JB|              203|    172|       0|      -6|   HOU| JFK|    1428|     6|     25|        0|            null|       0|\n",
      "|2011|    1|        31|        1|   1737|   2003|           CO|      667| N57869|              266|    249|       0|      12|   IAH| SEA|    1874|     6|     11|        0|            null|       0|\n",
      "|2011|    1|        31|        1|   1034|   1348|           CO|      686| N56859|              134|    105|       0|      -1|   IAH| MCO|     853|     8|     21|        0|            null|       0|\n",
      "|2011|    1|        31|        1|   1144|   1241|           CO|      741| N37290|               57|     32|       0|      -1|   IAH| AUS|     140|     5|     20|        0|            null|       0|\n",
      "|2011|    1|        31|        1|    842|   1027|           CO|      799| N57855|              165|    133|       0|      -3|   IAH| DEN|     862|    17|     15|        0|            null|       0|\n",
      "|2011|    1|        31|        1|   1252|   1559|           CO|     1757| N18622|              127|     85|       0|       2|   IAH| ATL|     689|    17|     25|        0|            null|       0|\n",
      "|2011|    1|        30|        7|    900|   1101|           CO|      220| N29124|              181|    153|       0|      -5|   IAH| PHX|    1009|     9|     19|        0|            null|       0|\n",
      "|2011|    1|        30|        7|   2106|   2212|           CO|      511| N54711|               66|     53|       0|      11|   IAH| MFE|     316|     4|      9|        0|            null|       0|\n",
      "|2011|    1|        30|        7|   1452|   1610|           CO|      697| N38403|              198|    177|       0|      12|   IAH| LAS|    1222|     5|     16|        0|            null|       0|\n",
      "|2011|    1|        30|        7|   1342|   1648|           CO|      786| N75853|              126|    107|       0|       7|   IAH| MCO|     853|     6|     13|        0|            null|       0|\n",
      "|2011|    1|        30|        7|   1102|   1443|           CO|      788| N39728|              161|    138|       0|      -3|   IAH| DTW|    1076|     7|     16|        0|            null|       0|\n",
      "|2011|    1|        30|        7|   1540|   1728|           CO|      795| N26226|              228|    204|       0|       0|   IAH| LAX|    1379|     7|     17|        0|            null|       0|\n",
      "|2011|    1|        30|        7|   1603|   1820|           CO|     1070| N73406|              257|    224|       0|       3|   IAH| SFO|    1635|     9|     24|        0|            null|       0|\n",
      "|2011|    1|        30|        7|   1323|   1639|           CO|     1548| N37408|              136|    109|       0|       8|   IAH| FLL|     965|     4|     23|        0|            null|       0|\n",
      "+----+-----+----------+---------+-------+-------+-------------+---------+-------+-----------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+\n",
      "|Dest|\n",
      "+----+\n",
      "| MSY|\n",
      "| DEN|\n",
      "| DAL|\n",
      "| ATL|\n",
      "| ORD|\n",
      "| LAX|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "hflight.withColumn(\"ArrDelay\", expr(\"ArrDelay == 0\")).show()\n",
    "hflight.where(expr(\"ArrDelay == 0\")).show()\n",
    "\n",
    "target = hflight.withColumn(\"count\", expr(\"ArrDelay >=5\")).\\\n",
    "    where(expr(\"count == True\")).\\\n",
    "    groupby(\"Dest\").\\\n",
    "    count().\\\n",
    "    filter(expr(\"count >= 2000\")).select(\"Dest\").rdd.flatMap(lambda x : x).collect()\n",
    "\n",
    "\n",
    "hflight2 = hflight.where(col('Dest').isin(target))\n",
    "\n",
    "hflight2.select(\"Dest\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288a661d-4ea5-47c3-bef4-7a7a2f550e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### JOIN\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ba330a-826d-4787-94a0-a10c83f23451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========================================================(2 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "empDF.join( deptDF, empDF.emp_dept_id == deptDF.dept_id, \"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c30e4-d0ed-49a7-99bf-4eaa741faa83",
   "metadata": {},
   "source": [
    "## 문제 3\n",
    "### 위의 결과를 바탕으로 목적지  공항 별 경항횟수, 회항횟수\n",
    "### 운항 횟수를 구하시오\n",
    "### 운항 횟수는 결항과 회항을 제외한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cb91c-b9d8-4ab5-9fc6-46c658d2af67",
   "metadata": {},
   "source": [
    "## 문제3 해설"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "991a1fb2-a170-47e0-bfcf-359823ebf6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hflight4 = spark.read.parquet(\"hdfs://namenode:8020/20240619/scw/hflight_ex3.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47343a6f-ee17-4a54-985b-28354a0c7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hflight4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf87a4f0-7ef6-4cad-b3e8-939bd4cc6fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "url = \"jdbc:mysql://43.202.5.70:3306/encore_web\"\n",
    "properties = {\n",
    "    \"user\": \"class5\",\n",
    "    \"password\": 'EnCoRo!23',\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "a = hflight4.groupBy(\"Dest\").sum(\"Cancelled\")\n",
    "b = hflight4.groupBy(\"Dest\").sum(\"Diverted\")\n",
    "c = hflight4.groupBy(\"Dest\").count()\n",
    "\n",
    "result = a.join(b, on=\"Dest\", how='left').\\\n",
    "    join(c, on=\"Dest\", how='left').\\\n",
    "    withColumnRenamed(\"sum(Cancelled)\", \"Cancelled\").\\\n",
    "    withColumnRenamed(\"sum(Diverted)\", \"Diverted\").\\\n",
    "    withColumn(\"air\", expr(\"count - Cancelled - Diverted\")).\\\n",
    "    drop(\"count\").\\\n",
    "    write.jdbc(url=url, table='hiyo', mode='append', properties=properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955708e2-466a-4303-a7b6-87484b355d50",
   "metadata": {},
   "source": [
    "# 하둡 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288888b5-5ad1-4a7a-8a64-9abf408076b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://43.202.5.70:3306/encore_web\") \\\n",
    "    .option(\"dbtable\",\"encore_question\") \\\n",
    "    .option(\"numPartitions\",5) \\\n",
    "    .option(\"user\", \"class5\") \\\n",
    "    .option(\"password\", 'EnCoRo!23') \\\n",
    "    .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb45f71-3c20-4027-896c-ba15d1803145",
   "metadata": {},
   "source": [
    "### df = spark.read **:\n",
    "\n",
    "df는 우리가 데이터를 저장할 변수의 이름\n",
    "spark.read는 데이터를 읽어오는 시작점\n",
    "\n",
    "### .format(\"jdbc\") **:\n",
    "\n",
    "jdbc는 Java Database Connectivity의 줄임말로, 데이터베이스에 연결하는 방법\n",
    "이 코드는 MySQL 데이터베이스에서 데이터를 읽어올 수 있음\n",
    "\n",
    "### .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") **:\n",
    "\n",
    "driver는 MySQL 데이터베이스에 연결할 때 사용하는 특별한 프로그램\n",
    "com.mysql.cj.jdbc.Driver는 MySQL에 연결할 수 있게 해주는 드라이버의 이름\n",
    "\n",
    "### .option(\"url\", \"jdbc:mysql://43.202.5.70:3306/encore_web\") **:\n",
    "\n",
    "url은 우리가 연결할 MySQL 데이터베이스의 주소\n",
    "jdbc:mysql://43.202.5.70:3306/encore_web는 \"43.202.5.70\"이라는 컴퓨터에 있는 \"encore_web\"라는 데이터베이스를 의미\n",
    "\n",
    "### .option(\"dbtable\",\"encore_question\") **:\n",
    "\n",
    "dbtable은 데이터베이스 안에 있는 테이블 이름\n",
    "encore_question 테이블에서 데이터를 읽어옴\n",
    "\n",
    "### .option(\"numPartitions\",5) **:\n",
    "\n",
    "numPartitions는 데이터를 몇 개의 조각으로 나눌지 정함\n",
    "데이터를 5개의 조각으로 나눌 거예요. 이렇게 하면 여러 컴퓨터가 동시에 데이터를 읽을 수 있음\n",
    "\n",
    "### .option(\"user\", \"class5\") **:\n",
    "\n",
    "user는 데이터베이스에 접속할 때 사용하는 사용자 이름\n",
    "여기서는 \"class5\"라는 사용자를 사용함\n",
    "\n",
    "### .option(\"password\", 'EnCoRo!23') **:\n",
    "\n",
    "password는 데이터베이스에 접속할 때 사용하는 비밀번호\n",
    "여기서는 'EnCoRo!23'이라는 비밀번호를 사용함\n",
    "\n",
    "### .load():\n",
    "\n",
    "load()는 지금까지 설정한 모든 정보를 사용해서 실제로 데이터를 읽어오는 명령\n",
    "이렇게 하면 df라는 변수에 데이터베이스에서 읽어온 데이터가 저장됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12013413-2b1a-4c6e-b54f-28670b2d7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----------------------+--------------------+---------+\n",
      "| id|                 subject|                content|         create_date|quthor_id|\n",
      "+---+------------------------+-----------------------+--------------------+---------+\n",
      "|  1|                       d|                      d|2024-05-14 06:37:...|        1|\n",
      "|  2|                       d|                      d|2024-05-14 06:37:...|        1|\n",
      "|  3|                       d|                      d|2024-05-14 06:37:...|        1|\n",
      "|  4|                 sdfasdf|                 fasadf|2024-05-14 06:38:...|        1|\n",
      "|  5|                    우와|                   우와|2024-05-14 06:38:...|        1|\n",
      "|  6|                 sdfasdf|                 fasadf|2024-05-14 06:38:...|        1|\n",
      "|  8|                    으오|                   ㅇㅇ|2024-05-14 06:39:...|        1|\n",
      "|  9|    첫글을 빼앗겼네요...|                   ㅠㅠ|2024-05-14 06:39:...|        1|\n",
      "| 10|                    집에|          가고싶어요...|2024-05-14 06:39:...|        1|\n",
      "| 11|    첫글을 빼앗겼네요...|                   ㅠㅠ|2024-05-14 06:39:...|        1|\n",
      "| 12|                    으오|                   ㅇㅇ|2024-05-14 06:39:...|        1|\n",
      "| 13|                     qqq|                     qq|2024-05-14 06:40:...|        1|\n",
      "| 15|내일은 부처님 오시는 날~|               쉬어요~~|2024-05-14 06:40:...|        1|\n",
      "| 16|                    집에|          가고싶어요...|2024-05-14 06:40:...|        1|\n",
      "| 17|                가나다라|                 마바사|2024-05-14 06:41:...|        1|\n",
      "| 18|               'wefopfew|   efjiow;joiwefefjiwo;|2024-05-14 06:41:...|        1|\n",
      "| 19| 오늘은 집에 일찍 가는날|오늘은 집에 일찍 가는날|2024-05-14 06:41:...|        1|\n",
      "| 20|  저녁 메뉴 추천해주세요|                뭐 먹징|2024-05-14 06:41:...|        1|\n",
      "| 21|                     휴~|      드디어 들어옴ㅜㅜ|2024-05-14 06:12:...|        1|\n",
      "| 22|              ㅎㅎㅎㅎㅎ|               하하하하|2024-05-14 06:42:...|        1|\n",
      "+---+------------------------+-----------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674657bf-0025-4872-863e-3cacdaa3d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "import subprocess\n",
    "\n",
    "classpath = subprocess.Popen([\"/home/hadoop/hadoop/bin/hdfs\", \"classpath\", \"--glob\"], stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ[\"CLASSPATH\"] = classpath.decode(\"utf-8\")\n",
    "hdfs = fs.HadoopFileSystem(host='192.168.0.160', port=8020, user='hadoop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb1e42a-c140-4a71-9095-ac7631ce4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4155e42f-7f3a-4a37-83e9-fc348d586627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----------------------+--------------------+---------+-----+\n",
      "| id|                 subject|                content|         create_date|quthor_id|dummy|\n",
      "+---+------------------------+-----------------------+--------------------+---------+-----+\n",
      "|  1|                       d|                      d|2024-05-14 06:37:...|        1| 하하|\n",
      "|  2|                       d|                      d|2024-05-14 06:37:...|        1| 하하|\n",
      "|  3|                       d|                      d|2024-05-14 06:37:...|        1| 하하|\n",
      "|  4|                 sdfasdf|                 fasadf|2024-05-14 06:38:...|        1| 하하|\n",
      "|  5|                    우와|                   우와|2024-05-14 06:38:...|        1| 하하|\n",
      "|  6|                 sdfasdf|                 fasadf|2024-05-14 06:38:...|        1| 하하|\n",
      "|  8|                    으오|                   ㅇㅇ|2024-05-14 06:39:...|        1| 하하|\n",
      "|  9|    첫글을 빼앗겼네요...|                   ㅠㅠ|2024-05-14 06:39:...|        1| 하하|\n",
      "| 10|                    집에|          가고싶어요...|2024-05-14 06:39:...|        1| 하하|\n",
      "| 11|    첫글을 빼앗겼네요...|                   ㅠㅠ|2024-05-14 06:39:...|        1| 하하|\n",
      "| 12|                    으오|                   ㅇㅇ|2024-05-14 06:39:...|        1| 하하|\n",
      "| 13|                     qqq|                     qq|2024-05-14 06:40:...|        1| 하하|\n",
      "| 15|내일은 부처님 오시는 날~|               쉬어요~~|2024-05-14 06:40:...|        1| 하하|\n",
      "| 16|                    집에|          가고싶어요...|2024-05-14 06:40:...|        1| 하하|\n",
      "| 17|                가나다라|                 마바사|2024-05-14 06:41:...|        1| 하하|\n",
      "| 18|               'wefopfew|   efjiow;joiwefefjiwo;|2024-05-14 06:41:...|        1| 하하|\n",
      "| 19| 오늘은 집에 일찍 가는날|오늘은 집에 일찍 가는날|2024-05-14 06:41:...|        1| 하하|\n",
      "| 20|  저녁 메뉴 추천해주세요|                뭐 먹징|2024-05-14 06:41:...|        1| 하하|\n",
      "| 21|                     휴~|      드디어 들어옴ㅜㅜ|2024-05-14 06:12:...|        1| 하하|\n",
      "| 22|              ㅎㅎㅎㅎㅎ|               하하하하|2024-05-14 06:42:...|        1| 하하|\n",
      "+---+------------------------+-----------------------+--------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"dummy\", lit(\"하하\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd97f571-c620-4d5b-b926-4b26353d9943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.write.mode(\"overwrite\").csv(\"hdfs://namenode:8020/20240619/scw/my3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a1543c0-2f8e-429f-88d3-cd0b0bddd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn(\"dummy2\", lit(\"맨유는 개못해\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d003c47-6e1e-4b1a-87c7-ce2f660e01f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 해당 경로에 폴더가 없어도 생성해줌\n",
    "df3.write.parquet(\"hdfs://namenode:8020/20240619/maenchester/liverpool.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26758350-7653-4b14-973e-269eabfc30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= spark.read.parquet(\"hdfs://namenode:8020/20240619/maenchester/liverpool.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acc5e984-1075-4fa9-b546-7e6ebba4efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----------------------+--------------------+---------+-----+-------------+\n",
      "| id|                 subject|                content|         create_date|quthor_id|dummy|       dummy2|\n",
      "+---+------------------------+-----------------------+--------------------+---------+-----+-------------+\n",
      "|  1|                       d|                      d|2024-05-14 06:37:...|        1| 하하|맨유는 개못해|\n",
      "|  2|                       d|                      d|2024-05-14 06:37:...|        1| 하하|맨유는 개못해|\n",
      "|  3|                       d|                      d|2024-05-14 06:37:...|        1| 하하|맨유는 개못해|\n",
      "|  4|                 sdfasdf|                 fasadf|2024-05-14 06:38:...|        1| 하하|맨유는 개못해|\n",
      "|  5|                    우와|                   우와|2024-05-14 06:38:...|        1| 하하|맨유는 개못해|\n",
      "|  6|                 sdfasdf|                 fasadf|2024-05-14 06:38:...|        1| 하하|맨유는 개못해|\n",
      "|  8|                    으오|                   ㅇㅇ|2024-05-14 06:39:...|        1| 하하|맨유는 개못해|\n",
      "|  9|    첫글을 빼앗겼네요...|                   ㅠㅠ|2024-05-14 06:39:...|        1| 하하|맨유는 개못해|\n",
      "| 10|                    집에|          가고싶어요...|2024-05-14 06:39:...|        1| 하하|맨유는 개못해|\n",
      "| 11|    첫글을 빼앗겼네요...|                   ㅠㅠ|2024-05-14 06:39:...|        1| 하하|맨유는 개못해|\n",
      "| 12|                    으오|                   ㅇㅇ|2024-05-14 06:39:...|        1| 하하|맨유는 개못해|\n",
      "| 13|                     qqq|                     qq|2024-05-14 06:40:...|        1| 하하|맨유는 개못해|\n",
      "| 15|내일은 부처님 오시는 날~|               쉬어요~~|2024-05-14 06:40:...|        1| 하하|맨유는 개못해|\n",
      "| 16|                    집에|          가고싶어요...|2024-05-14 06:40:...|        1| 하하|맨유는 개못해|\n",
      "| 17|                가나다라|                 마바사|2024-05-14 06:41:...|        1| 하하|맨유는 개못해|\n",
      "| 18|               'wefopfew|   efjiow;joiwefefjiwo;|2024-05-14 06:41:...|        1| 하하|맨유는 개못해|\n",
      "| 19| 오늘은 집에 일찍 가는날|오늘은 집에 일찍 가는날|2024-05-14 06:41:...|        1| 하하|맨유는 개못해|\n",
      "| 20|  저녁 메뉴 추천해주세요|                뭐 먹징|2024-05-14 06:41:...|        1| 하하|맨유는 개못해|\n",
      "| 21|                     휴~|      드디어 들어옴ㅜㅜ|2024-05-14 06:12:...|        1| 하하|맨유는 개못해|\n",
      "| 22|              ㅎㅎㅎㅎㅎ|               하하하하|2024-05-14 06:42:...|        1| 하하|맨유는 개못해|\n",
      "+---+------------------------+-----------------------+--------------------+---------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c7288-771f-4ca8-aca6-e46ae630e57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
